{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "bert_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ccbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from shutil import copyfile\n",
    "\n",
    "classes = ['has_codingsystem','code_of','has_code','negative']\n",
    "def read_sample(file):\n",
    "    df = pd.read_csv(file,sep='\\t')\n",
    "    file_dict = {'text':[],'label':[]}\n",
    "    file_dict['text']=df['Sentence']\n",
    "    file_dict['label']=df['Relation']\n",
    "    return file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e144f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"],padding=True,truncation=True,max_length=512)\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "device = torch.device(\"cuda\")\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'f1':f1_score(labels, predictions, average='weighted')}\n",
    "\n",
    "def train_main(output_dir,model):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=1e-6,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        save_strategy = \"epoch\",\n",
    "        metric_for_best_model = 'f1'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['valid'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics = compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    \n",
    "def evaluation(model):\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    for instance in (tokenized_dataset['test']):\n",
    "        inputs = tokenizer(instance['text'],padding=True,truncation=True,max_length=512, return_tensors=\"pt\").to(device)\n",
    "        labels = torch.tensor([1]).unsqueeze(0).to(device)\n",
    "        #print (labels)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        alllabels.append(instance['label'])\n",
    "        allpreds.append(list(logits[0]).index(max(logits[0])))\n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(alllabels, allpreds, target_names = classes, digits=4)\n",
    "    print (report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e7fd5",
   "metadata": {},
   "source": [
    "### human annotation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = './data/RE/human_annotated/train.csv'\n",
    "valid_files = './data/RE/human_annotated/dev.csv'\n",
    "test_files = './data/RE/human_annotated/test.csv'\n",
    "\n",
    "train_dict = read_sample(train_files)\n",
    "test_dict = read_sample(test_files)\n",
    "valid_dict = read_sample(valid_files)\n",
    "\n",
    "train = Dataset.from_dict(train_dict)\n",
    "test = Dataset.from_dict(test_dict)\n",
    "valid = Dataset.from_dict(valid_dict)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "all_dataset = imdb\n",
    "all_dataset['train']=train\n",
    "all_dataset['valid']=valid\n",
    "all_dataset['test']=test\n",
    "all_dataset.pop('unsupervised')\n",
    "\n",
    "tokenized_dataset = all_dataset.map(preprocess_function, batched=True)\n",
    "output_dir = \"./models/RE_BiomedBERT/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(bert_name, num_labels=len(classes)).to(device)\n",
    "train_main(output_dir,model)\n",
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf07efa",
   "metadata": {},
   "source": [
    "### synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = './data/RE/synthetic/train.csv'\n",
    "valid_files = './data/RE/synthetic/valid.csv'\n",
    "test_files = './data/RE/human_annotated/test.csv'\n",
    "\n",
    "train_dict = read_sample(train_files)\n",
    "valid_dict = read_sample(valid_files)\n",
    "test_dict = read_sample(test_files)\n",
    "\n",
    "train = Dataset.from_dict(train_dict)\n",
    "valid = Dataset.from_dict(valid_dict)\n",
    "test = Dataset.from_dict(test_dict)\n",
    "\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "all_dataset = imdb\n",
    "all_dataset['train']=train\n",
    "all_dataset['valid']=valid\n",
    "all_dataset['test']=test\n",
    "all_dataset.pop('unsupervised')\n",
    "\n",
    "tokenized_dataset = all_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34763ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "classes = ['has_codingsystem','code_of','has_code','negative']\n",
    "\n",
    "print (Counter(train['label']))\n",
    "print (Counter(valid['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./models/RE_BiomedBERT_synthetic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a025e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(bert_name, num_labels=len(classes)).to(device)\n",
    "train_main(output_dir,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = './data/RE/human_annotated/train.csv'\n",
    "valid_files = './data/RE/human_annotated/dev.csv'\n",
    "test_files = './data/RE/human_annotated/test.csv'\n",
    "\n",
    "train_dict = read_sample(train_files)\n",
    "test_dict = read_sample(test_files)\n",
    "valid_dict = read_sample(valid_files)\n",
    "\n",
    "train = Dataset.from_dict(train_dict)\n",
    "test = Dataset.from_dict(test_dict)\n",
    "valid = Dataset.from_dict(valid_dict)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "all_dataset = imdb\n",
    "all_dataset['train']=train\n",
    "all_dataset['valid']=valid\n",
    "all_dataset['test']=test\n",
    "all_dataset.pop('unsupervised')\n",
    "tokenized_dataset = all_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir, num_labels=len(classes)).to(device)\n",
    "output_dir = \"./models/RE_BiomedBERT_synthetic_continual/\"\n",
    "train_main(output_dir,model)\n",
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = []\n",
    "alllabels = []\n",
    "for instance in (tokenized_dataset['test']):\n",
    "    inputs = tokenizer(instance['text'],padding=True,truncation=True,max_length=512, return_tensors=\"pt\").to(device)\n",
    "    labels = torch.tensor([1]).unsqueeze(0).to(device)\n",
    "    #print (labels)\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    alllabels.append(instance['label'])\n",
    "    allpreds.append(list(logits[0]).index(max(logits[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa893869",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./RE_output.txt','w') as f:\n",
    "    for text, gold, prediction in zip(test_dict['text'],alllabels,allpreds):\n",
    "        f.write(text+'\\t'+classes[gold]+'\\t'+classes[prediction]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cae835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a656ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
